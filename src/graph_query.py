# src/graph_query.py
from .config import OPENAI_API_KEY, OPENAI_CHAT_MODEL
from .embeddings import embed_texts
from .neo4j_client import run_cypher
from typing import Dict
import logging

logger = logging.getLogger(__name__)

def nl_query_to_cypher(nl: str, top_k: int = 5) -> Dict:
    """Try to produce a safe, read-only Cypher via LLM. If LLM not available, fallback to semantic retrieval."""
    if OPENAI_API_KEY:
        try:
            import openai
            openai.api_key = OPENAI_API_KEY
            system = (
                "You are an assistant that converts user questions into safe, read-only Neo4j Cypher queries. "
                "Only produce a MATCH/RETURN query against Chunk nodes, and limit results."
            )
            prompt = f"User question: {nl}\nProduce only a Cypher query that MATCHes Chunk nodes and RETURNS c.id, c.text LIMIT 20."
            resp = openai.ChatCompletion.create(model=OPENAI_CHAT_MODEL, messages=[{'role':'system','content':system},{'role':'user','content':prompt}], temperature=0.0, max_tokens=300)
            cypher = resp['choices'][0]['message']['content'].strip()
            if cypher.upper().startswith('MATCH') or 'RETURN' in cypher.upper():
                return {'cypher': cypher, 'explanation': 'Generated by LLM'}
        except Exception as e:
            logger.exception('LLM->Cypher failed: %s', e)
    # fallback: semantic retrieval -> build safe cypher with nearest ids
    emb = embed_texts([nl])[0]
    rows = run_cypher('MATCH (c:Chunk) WHERE exists(c.embedding) RETURN c.id AS id, c.embedding AS emb')
    import numpy as _np
    items = []
    for r in rows:
        items.append((r['id'], _np.array(r.get('emb') or [], dtype=float)))
    if not items:
        return {'cypher': '-- no data --', 'explanation': 'No nodes in graph. Ingest notes first.'}
    q = _np.array(emb, dtype=float)
    scores = []
    for id_, v in items:
        denom = (_np.linalg.norm(q)*_np.linalg.norm(v))
        sim = float(_np.dot(q, v)/denom) if denom!=0 else 0.0
        scores.append((id_, sim))
    ranked = sorted(scores, key=lambda x: -x[1])[:top_k]
    ids_list = [f'"{i[0]}"' for i in ranked]
    cy = f'MATCH (c:Chunk) WHERE c.id IN [{", ".join(ids_list)}] RETURN c.id AS id, c.text AS text LIMIT {top_k}'
    return {'cypher': cy, 'explanation': 'Fallback: semantic retrieval based on embeddings'}

def run_cypher_query_and_summarize(cypher: str, question: str) -> Dict:
    # Simple safety: block write keywords
    banned = ['CREATE','MERGE','DELETE','SET','REMOVE','DROP','CALL']
    up = cypher.upper()
    if any(b in up for b in banned):
        return {'error': 'Unsafe cypher detected; execution blocked.', 'rows': []}
    rows = run_cypher(cypher)
    texts = [r.get('text','') for r in rows][:8]
    answer = ''
    if OPENAI_API_KEY and texts:
        try:
            import openai
            openai.api_key = OPENAI_API_KEY
            prompt = 'Given the passages below, answer the question concisely and cite which passages you used.\n\nQuestion: ' + question + '\n\nPassages:\n'
            for i,t in enumerate(texts):
                prompt += f'[{i}] ' + t + '\n'
            resp = openai.ChatCompletion.create(model=OPENAI_CHAT_MODEL, messages=[{'role':'user','content':prompt}], temperature=0.0, max_tokens=400)
            answer = resp['choices'][0]['message']['content'].strip()
        except Exception as e:
            logger.exception('LLM summarization failed: %s', e)
            answer = '\n\n'.join(texts)
    else:
        answer = '\n\n'.join(texts) if texts else 'No relevant notes found.'
    table_rows = [{'id': r.get('id'), 'text': (r.get('text')[:300] + ('...' if len(r.get('text',''))>300 else ''))} for r in rows]
    return {'answer': answer, 'rows': table_rows}
